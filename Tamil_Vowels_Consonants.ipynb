{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-dataloader-code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWa-q3j9tmS2",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5H2GSuoQVyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3pab_tX5P_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFvF_HIEHLBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#num_classes=10\n",
        "_imported=False\n",
        "_loaded = False\n",
        "_trained = False\n",
        "epochs = 10\n",
        "\n",
        "# transform = None\n",
        "# train_loader = None\n",
        "# validation_loader= None\n",
        "# net  = None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqXRcv8sQfm6",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PeozigfFQ850",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#For converting the dataset to torchvision dataset format\n",
        "class VowelConsonantDataset(Dataset):\n",
        "    def __init__(self, file_path,train=True,transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_path=file_path\n",
        "        self.train=train\n",
        "        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n",
        "        self.len = len(self.file_names)\n",
        "        if self.train:\n",
        "            self.classes_mapping=self.get_classes()\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        file_name=self.file_names[index]\n",
        "        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "            #image_data = image_data.view(image_data.size(0), -1)\n",
        "            \n",
        "            #image_data = image_data.reshape(64*64*3,1)\n",
        "        if self.train:\n",
        "            file_name_splitted=file_name.split(\"_\")\n",
        "            Y1 = self.classes_mapping[file_name_splitted[0]]\n",
        "            Y2 = self.classes_mapping[file_name_splitted[1]]\n",
        "            z1,z2=torch.zeros(10),torch.zeros(10)\n",
        "            z1[Y1-10],z2[Y2]=1,1\n",
        "            #label=torch.stack([z1,z2])\n",
        "            label_vowels=z1\n",
        "            label_consonants=z2\n",
        "\n",
        "            return image_data, label_vowels,label_consonants\n",
        "\n",
        "        else:\n",
        "            return image_data, file_name\n",
        "          \n",
        "    def pil_loader(self,path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "      \n",
        "    def get_classes(self):\n",
        "        classes=[]\n",
        "        for name in self.file_names:\n",
        "            name_splitted=name.split(\"_\")\n",
        "            classes.extend([name_splitted[0],name_splitted[1]])\n",
        "        classes=list(set(classes))\n",
        "        classes_mapping={}\n",
        "        for i,cl in enumerate(sorted(classes)):\n",
        "            classes_mapping[cl]=i\n",
        "        return classes_mapping\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT6_4D57Qmiz",
        "colab_type": "text"
      },
      "source": [
        "# Load Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjPiv76DIeC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1lafkZ1MHBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kaggleImport():\n",
        "  \n",
        "  # Import kaggle.json from google drive\n",
        "  # This snippet will output a link which needs authentication from any google account\n",
        "  from googleapiclient.discovery import build\n",
        "  import io, os\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  drive_service = build('drive', 'v3')\n",
        "  results = drive_service.files().list(\n",
        "      q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "  kaggle_api_key = results.get('files', [])\n",
        "  print(kaggle_api_key)\n",
        "  #filename = \"/content/.kaggle/kaggle.json\"\n",
        "  filename = \"/root/.kaggle/kaggle.json\"\n",
        "  os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "  request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "  fh = io.FileIO(filename, 'wb')\n",
        "  downloader = MediaIoBaseDownload(fh, request)\n",
        "  done = False\n",
        "  while done is False:\n",
        "      status, done = downloader.next_chunk()\n",
        "      print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "  os.chmod(filename, 600)\n",
        "  _imported=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3CIVHiwM-QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData():\n",
        "  \n",
        "  # Use this command in new cell\n",
        "  #!kaggle datasets download -d stanfordu/street-view-house-numbers -w -f street-view-house-numbers.zip\n",
        "  !kaggle competitions download -c padhai-hindi-vowel-consonant-classification\n",
        "#   !kaggle competitions download -c padhai-tamil-vowel-consonant-classification\n",
        "\n",
        "\n",
        "  # Unzip the data\n",
        "  #!unzip street-view-house-numbers.zip\n",
        "  !unzip train.zip\n",
        "\n",
        "  !unzip test.zip\n",
        "  _loaded=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs1-6ampRBOD",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Training & Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-xb9vWnVHo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224), \n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#     ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224), \n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#     ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZmaY_CVKtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "#                                         download=True, \n",
        "#                                         transform=transform_train)\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
        "#                                         download=True, \n",
        "#                                         transform=transform_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYM_13kwVQ4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDlf5fyuhiEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=30\n",
        "train_on_gpu = torch.cuda.is_available()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2cOje6hS_yA",
        "colab": {}
      },
      "source": [
        "def prepareTrainingData():\n",
        "  \n",
        "  global transform\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor()])\n",
        "\n",
        "  full_data = VowelConsonantDataset(\"train\",train=True,transform=transform)\n",
        "  #full_data = VowelConsonantDataset(\"../input/train/train\",train=True,transform=transform)\n",
        "  train_size = int(0.9 * len(full_data))\n",
        "  validation_size = int(0.1 * len(full_data))\n",
        "#   test_size = int(0.25 * len(full_data))\n",
        "  #temp1_size= int(0.002 * len(full_data))\n",
        "  #test_size = len(full_data) - train_size\n",
        "  #temp1, temp2 =  random_split(full_data, [temp1_size, len(full_data)-temp1_size])\n",
        "  #len(temp1)\n",
        "  train_data, validation_data= random_split(full_data, [train_size, validation_size])\n",
        "  #train_data, validation_data = random_split(temp1, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "  validation_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=True)\n",
        "#   test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)\n",
        "  return transform, train_loader, validation_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uzv8q9j63A_L",
        "colab": {}
      },
      "source": [
        "def prepareTestData(transform):\n",
        "  \n",
        "  test_data = VowelConsonantDataset(\"test\",train=False,transform=transform)\n",
        "# ../input/train/test\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size,shuffle=False)\n",
        "  return test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCa-db1bWePh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# |dataiter = iter(train_loader)\n",
        "# images, label_vowels, labels_consonants = dataiter.next()\n",
        "# print(label_vowels.shape)\n",
        "# for i, img in enumerate(images,0):\n",
        "#   #img = images[i]\n",
        "#   print(type(img))\n",
        "#   npimg = img.numpy()\n",
        "#   print(npimg.shape)\n",
        "#   #print (label_vowels[i,0], labels[i,1])\n",
        "#   print(label_vowels[i,0])\n",
        "#   npimg = np.transpose(npimg, (1, 2, 0))\n",
        "#   print(npimg.shape)\n",
        "#   plt.figure(figsize = (1,1))\n",
        "#   plt.imshow(npimg)\n",
        "#   plt.show()\n",
        "# #images = images.view(images.size(0), -1)\n",
        "# print(images.shape)\n",
        "\n",
        "# print(label_vowels[10,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYro0sN5LwN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print (labels[:,0])\n",
        "# type(labels[:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ8nM85O8jsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showImage(images):\n",
        "\n",
        "  for i, img in enumerate(images,0):\n",
        "\n",
        "    print(type(img))\n",
        "    npimg = img.cpu().numpy()\n",
        "    #print(npimg.shape)\n",
        "    #print (labels[i,0], labels[i,1])\n",
        "\n",
        "    npimg = np.transpose(npimg, (1, 2, 0))\n",
        "    #print(npimg.shape)\n",
        "    plt.figure(figsize = (1,1))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Vh2jnVXM6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_vowels = label_vowels.view(label_vowels.size(0),-1)\n",
        "\n",
        "# for i in range(10,20):\n",
        "#   img = images[i]\n",
        "#   print(type(img))\n",
        "#   npimg = img.numpy()\n",
        "#   print(npimg.shape)\n",
        "#   #print (labels[i,0], labels[i,1])\n",
        "#   print(label_vowels[i])\n",
        "#   npimg = np.transpose(npimg, (1, 2, 0))\n",
        "#   print(npimg.shape)\n",
        "#   plt.figure(figsize = (1,1))\n",
        "#   plt.imshow(npimg)\n",
        "#   plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKZjH7LyRIhG",
        "colab_type": "text"
      },
      "source": [
        "# Class Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG7nFlHjg4u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(0)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(12288,1024)  ,\n",
        "        nn.LeakyReLU(), \n",
        "        nn.Linear(1024,64), \n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(64,4),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(4,10),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.net(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTVXCX7MdqRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(dataloader, net, flag):\n",
        "    total , correct, total_vowels, correct_vowels, total_consonants, correct_consonants = 0, 0, 0, 0, 0, 0\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(device)\n",
        "    for i,data in enumerate(dataloader,0):\n",
        "        #inputs = inputs.view(inputs.size(0), -1)\n",
        "        #label_vowels = labels[0,0]\n",
        "        inputs, label_vowels, label_consonants = data\n",
        "\n",
        "        label_vowels= label_vowels.view(label_vowels.size(0),-1)\n",
        "        label_consonants= label_consonants.view(label_consonants.size(0),-1)\n",
        "        inputs, label_vowels,label_consonants = inputs.to(device), label_vowels.to(device, dtype=torch.int64), label_consonants.to(device, dtype=torch.int64)\n",
        "#         for i, lbl in enumerate(labels,0):\n",
        "#           print(lbl)\n",
        "\n",
        "#        showImage(inputs)   \n",
        "        \n",
        "        outputs = net(inputs)\n",
        "#         for j,pred in enumerate(outputs.data,0):\n",
        "#           print(pred)\n",
        "        #_, pred = torch.max(outputs.data, 1)\n",
        "        _, pred_vowels = torch.max(outputs[0], 1)\n",
        "        _, pred_consonants = torch.max(outputs[1], 1)\n",
        "      \n",
        "#         result_arr.append(tf.strings.as_string(file_name), \"V\" * tf.strings.as_string(pred_vowels) )\n",
        "#         vowel = tf.concat(\"V\",tf.strings.as_string( pred_vowels.cpu()))    \n",
        "#         consonants = tf.concat(\"C\",tf.strings.as_string(pred_consonants.cpu()))\n",
        "#         print (\"vowel\", vowel)\n",
        "#         print (\"consonants\", consonants)\n",
        "\n",
        "#        if flag == 0:\n",
        "    \n",
        "        total_vowels += label_vowels.size(0)\n",
        "        correct_vowels += (pred_vowels == torch.max(label_vowels, 1)[1]).sum().item()\n",
        "#        else:\n",
        "        total_consonants += label_consonants.size(0)\n",
        "        correct_consonants += (pred_consonants == torch.max(label_consonants, 1)[1]).sum().item()\n",
        "    \n",
        "        total +=label_vowels.size(0)\n",
        "        pred_vowels == torch.max(label_vowels, 1)[1]\n",
        "        pred_consonants == torch.max(label_consonants,1)[1]\n",
        "        result_vowels = (pred_vowels == torch.max(label_vowels, 1)[1])\n",
        "        result_consonants =(pred_consonants == torch.max(label_consonants,1)[1])\n",
        "\n",
        "#         print(\"vowels\", pred_vowels, torch.max(label_vowels, 1)[1], result_vowels)\n",
        "#         print(\"consonants\", pred_consonants, torch.max(label_consonants, 1)[1], result_consonants)\n",
        "        \n",
        "        correct += (result_vowels & result_consonants).sum().item()\n",
        "        #correct += ((pred_vowels == torch.max(label_vowels, 1)[1]) and (pred_consonants == torch.max(label_consonants,1)[1])).sum().item()\n",
        "#         print (\"total & correct\",i, \"***\",  total, correct)\n",
        "        del inputs, label_vowels, label_consonants, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return 100 * correct_vowels / total_vowels,100 * correct_consonants / total_consonants, 100 * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lXEArjKfNHa",
        "colab_type": "code",
        "outputId": "f7a809a9-1aed-4ede-c3e8-c602b9b309e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kg6mC8O50FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(dataloader, net):\n",
        "    result_arr = []\n",
        "    file_arr=[]\n",
        "    vowel_consonant=[]\n",
        "    filename=[]\n",
        "    \n",
        "    for i,data in enumerate(dataloader,0):\n",
        "        inputs, file_name = data\n",
        "        \n",
        "\n",
        "        inputs= inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, pred_vowels = torch.max(outputs[0], 1)\n",
        "        _, pred_consonants = torch.max(outputs[1], 1)\n",
        "          \n",
        "        vowel = np.char.add(np.full(pred_vowels.shape, \"V\").astype(str) , pred_vowels.cpu().numpy().astype(str))\n",
        "        consonant=np.char.add(np.full(pred_consonants.shape, \"_C\").astype(str) , pred_consonants.cpu().numpy().astype(str))\n",
        "#         np.insert(vowel_consonant,len(vowel_consonant), np.char.add(vowel,consonant))\n",
        "#         np.insert(filename,len(filename), file_name)\n",
        "        #np.concatenate((result,file_name), axis =1)\n",
        "        vowel_consonant  = np.char.add(vowel,consonant)\n",
        "        result_arr.append (vowel_consonant)\n",
        "        file_arr.append(file_name)\n",
        "    pred_list = []\n",
        "    print(\"file_arr\",file_arr)\n",
        "#     print('result_arr', result_arr)\n",
        "    \n",
        "    for sublist in result_arr:\n",
        "      for item in sublist:\n",
        "          pred_list.append(item)\n",
        "    for sublist in file_arr:\n",
        "      for item in sublist:\n",
        "          filename.append(item)\n",
        "    result =np.array([np.asarray(filename),np.asarray( pred_list)])\n",
        "    print (type(result))\n",
        "#     print(\"filename\",filename)\n",
        "#     print('pred_list', pred_list)\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3n7-IsL5rlw",
        "colab_type": "text"
      },
      "source": [
        "# CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs2KvF4c_Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(LeNet, self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, 5),         # (N, 3, 64, 64) -> (N,  6, 56, 56)\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(2, stride=2),  # (N, 6, 56, 56) -> (N,  6, 28, 28)\n",
        "            nn.Conv2d(6, 16, 5),        # (N, 6, 28, 28) -> (N, 16, 24, 24)  \n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(2, stride=2)   # (N,16, 24, 24) -> (N, 16, 12, 12)\n",
        "        )\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(2704,120),         # (N, 2704) -> (N, 120)\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n",
        "        )\n",
        "        self.fc_model1 = nn.Sequential(\n",
        "            nn.Linear(2704,120),         # (N, 2704) -> (N, 120)\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x1 = self.fc_model(x)\n",
        "        x2= self.fc_model1(x)\n",
        "        return torch.stack([x1,x2])      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1uR4r0q_LQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet_ReLU(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(LeNet_ReLU, self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, 5),         # (N, 3, 64, 64) -> (N,  6, 56, 56)\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2, stride=2),  # (N, 6, 56, 56) -> (N,  6, 28, 28)\n",
        "            nn.Conv2d(6, 16, 5),        # (N, 6, 28, 28) -> (N, 16, 24, 24)  \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2, stride=2)   # (N,16, 24, 24) -> (N, 16, 12, 12)\n",
        "        )\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(2704,120),         # (N, 2704) -> (N, 120)\n",
        "            nn.BatchNorm1d(120),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n",
        "            nn.BatchNorm1d(84),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n",
        "        )\n",
        "        self.fc_model1 = nn.Sequential(\n",
        "            nn.Linear(2704,120),         # (N, 2704) -> (N, 120)\n",
        "            nn.BatchNorm1d(120),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n",
        "            nn.BatchNorm1d(84),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x2= self.fc_model1(x)\n",
        "        x = self.fc_model(x)\n",
        "        return torch.stack([x,x2])\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6MzV1OFBAH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet_LeakyReLU(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(LeNet_LeakyReLU, self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, 5),         # (N, 3, 64, 64) -> (N,  6, 56, 56)\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(2, stride=2),  # (N, 6, 56, 56) -> (N,  6, 28, 28)\n",
        "            nn.Conv2d(6, 16, 5),        # (N, 6, 28, 28) -> (N, 16, 24, 24)  \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(2, stride=2)   # (N,16, 24, 24) -> (N, 16, 12, 12)\n",
        "        )\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(2704,1200),         # (N, 2704) -> (N, 1200)\n",
        "            nn.BatchNorm1d(1200),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(1200,600),          # (N, 1200) -> (N, 600)\n",
        "            nn.BatchNorm1d(600),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(600,200),          # (N, 600) -> (N, 200)\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(200,10)            # (N, 200)  -> (N, 10)\n",
        "        )\n",
        "        self.fc_model1 = nn.Sequential(\n",
        "            nn.Linear(2704,1000),         # (N, 2704) -> (N, 1000)\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(1000,500),         # (N, 1000) -> (N, 500)\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(500,100),         # (N, 500) -> (N, 100)\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(100,84),          # (N, 100) -> (N, 84)\n",
        "            nn.BatchNorm1d(84),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n",
        "        )\n",
        "#         self.fc_model = nn.Sequential(\n",
        "#             nn.Linear(2704,1024),         # (N, 2704) -> (N, 1200)\n",
        "#             nn.BatchNorm1d(1024),\n",
        "#             nn.Dropout(0.2),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(1024,10)          # (N, 1200) -> (N, 600)\n",
        "#         )\n",
        "#         self.fc_model1 = nn.Sequential(\n",
        "#             nn.Linear(2704,1024),         # (N, 2704) -> (N, 1000)\n",
        "#             nn.BatchNorm1d(1024),\n",
        "#             nn.Dropout(0.2),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(1024,512),         # (N, 1000) -> (N, 500)\n",
        "#             nn.BatchNorm1d(512),\n",
        "#             nn.Dropout(0.2),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(512,10)         # (N, 500) -> (N, 100)\n",
        "#         )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x2= self.fc_model1(x)\n",
        "        x = self.fc_model(x)\n",
        "        return torch.stack([x,x2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5kl69ddd73a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadModel(model,activation_fn, optimation_fn):\n",
        "  net=None\n",
        "  if (model == \"FF\"):\n",
        "    net=FeedForwardNetwork().to(device)\n",
        "  elif (model == \"CNN\"):\n",
        "    if (activation_fn == \"Tanh\"):\n",
        "      net=LeNet().to(device)\n",
        "    elif (activation_fn == \"ReLU\"):\n",
        "      net=LeNet_ReLU().to(device)\n",
        "    elif (activation_fn == \"LeakyReLU\"):\n",
        "      net=LeNet_LeakyReLU().to(device)\n",
        "  elif (model == \"VGG\"):\n",
        "    net = vggNet().to(device)\n",
        "  elif (model == \"ALEXNET\"):\n",
        "    net = alexNet().to(device)\n",
        "#     final_in_features = net.classifier[6].in_features\n",
        "#     mod_classifier = list(net.classifier.children())[:-1]\n",
        "#     mod_classifier.extend([nn.Linear(final_in_features, num_classes)])\n",
        "#     net.classifier = nn.Sequential(*mod_classifier)\n",
        "\n",
        "#     print(mod_classifier)\n",
        "  #loss_fn = nn.CrossEntropyLoss()\n",
        "  \n",
        "  #opt = optim.Adam(net.parameters())\n",
        "  #opt = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "  #opt=optim.RMSprop(net.parameters())\n",
        "  #opt = optim.Adagrad(net.parameters(),lr=0.5)\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1KcMYcsn_nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params(object):\n",
        "    def __init__(self, activation_fn,optim, model, epochs):\n",
        "        self.activation_fn = activation_fn\n",
        "        self.optim = optim\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zkakwIG2wZ",
        "colab_type": "text"
      },
      "source": [
        "# Large CNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "immr91VOUQRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PredictVowelsAndConsonants(args, net, train_loader, validation_loader):\n",
        "  print(args.model, args.activation_fn, args.optim)\n",
        "  \n",
        "  %%time  \n",
        "  loss1_arr = []\n",
        "  loss_epoch_arr1 = []\n",
        "  loss2_arr = []\n",
        "  loss_epoch_arr2 = []\n",
        "  max_epochs = args.epochs\n",
        "  opt = optim.Adam(net.parameters(),lr=0.001) #, weight_decay=1e-5 , lr=0.01 #, lr=1e-5, weight_decay=1e-5 (0.001)\n",
        "#   torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)?\n",
        "  #opt = optim.SGD(net.parameters(), lr=0.1, momentum=0.6)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "#   loss_fn = nn.MultiLabelSoftMarginLoss()\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#   print(device)\n",
        "  net = net.to(device)\n",
        "  for epoch in range(max_epochs):\n",
        "\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          inputs, label_vowels, label_consonants = data\n",
        "          #showImage(inputs[i])\n",
        "          #inputs = inputs.view(inputs.size(0), -1)\n",
        "          #label_vowels = labels[0]\n",
        "          label_vowels = label_vowels.view(label_vowels.size(0),-1)\n",
        "          label_consonants = label_consonants.view(label_consonants.size(0),-1)\n",
        "          inputs, label_vowels, label_consonants = inputs.to(device), label_vowels.to(device, dtype=torch.int64),label_consonants.to(device, dtype=torch.int64)\n",
        "\n",
        "          opt.zero_grad()\n",
        "\n",
        "          outputs = net(inputs)\n",
        "#           print(\"type\", type(outputs))\n",
        "          #print(outputs[0])\n",
        "          #_, pred= torch.max(outputs.data, 1)\n",
        "          _, pred_vowels= torch.max(outputs[0], 1)\n",
        "          _, pred_consonants= torch.max(outputs[1], 1)\n",
        "          #print(\"max\", pred,  torch.max(labels, 1)[1])\n",
        "          #loss = loss_fn(outputs, torch.max(label_vowels, 1)[1]) #+ loss_fn(outputs, torch.max(label_consonants, 1)[1])\n",
        "          \n",
        "#           print (\"output shape\", outputs[0].shape)\n",
        "#           print(\"inputs shape\", inputs.shape)\n",
        "#           print(\"labes shape\", label_vowels.shape, label_consonants.shape)\n",
        "#           print (\"label\",  outputs[0], torch.max(label_vowels, 1)[1])\n",
        "          loss1 = loss_fn(outputs[0], torch.max(label_vowels, 1)[1]) \n",
        "          loss2 = loss_fn(outputs[1], torch.max(label_consonants, 1)[1])\n",
        "          loss = loss1 + loss2\n",
        "          #print(loss1, loss2, loss)\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          loss1_arr.append(loss1.item())       \n",
        "          loss2_arr.append(loss2.item())       \n",
        "      loss_epoch_arr1.append(loss1.item())\n",
        "      loss_epoch_arr2.append(loss2.item())\n",
        "\n",
        "      training_accuracy_score = evaluation(train_loader,net,0)\n",
        "      validation_accuracy_score = evaluation(validation_loader,net,0)\n",
        "      del inputs, label_vowels, label_consonants, outputs\n",
        "      torch.cuda.empty_cache()\n",
        "      \n",
        "      print('Epoch: %d/%d, Overall Train acc: %0.2f,Overall Validation acc: %0.2f' % (epoch, max_epochs,training_accuracy_score[2],validation_accuracy_score[2]))\n",
        "  plt.figure()\n",
        "  plt.subplot(211)\n",
        "  plt.plot(loss_epoch_arr1)\n",
        "\n",
        "  plt.subplot(212)\n",
        "  \n",
        "  plt.plot(loss_epoch_arr2)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y_65Wp8P4r0",
        "colab_type": "code",
        "outputId": "5dccdb94-3b00-4cd7-96f4-44e74d6c4549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchvision import models\n",
        "vgg = models.vgg16_bn(pretrained=True) #(pretrained=True)\n",
        "print(vgg)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Dropout(p=0.5)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.5)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aNO6-54udy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhgbwE1YHGx3",
        "colab_type": "code",
        "outputId": "1a969606-71aa-4ff2-c8ca-d686a47bc36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "new_classifier = nn.Sequential(*list(vgg.classifier.children())[:-7])\n",
        "vgg.classifier = new_classifier\n",
        "print(vgg)\n",
        "vgg= vgg.to(device)\n",
        "# final_in_features = vgg.classifier[6].in_features\n",
        "# vgg.classifier[6] = nn.Linear(final_in_features, num_classes)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8iDAguyrQDI",
        "colab_type": "code",
        "outputId": "2648e4f3-6b86-47c7-c443-aba33420cbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "print(vgg)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVHJ_ZPOzlRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearNet(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(LinearNet, self).__init__()\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(25088,12288),         # (N, 2704) -> (N, 1200)\n",
        "            nn.BatchNorm1d(12288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(12288,4096),         # (N, 2704) -> (N, 1200)\n",
        "            nn.BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(4096,1024)         # (N, 2704) -> (N, 1200)\n",
        "            \n",
        "        )\n",
        "        self.fc_model1 = nn.Sequential(\n",
        "              nn.Linear(1024,10)\n",
        "#             nn.Linear(1024,512),         # (N, 1000) -> (N, 500)\n",
        "#             nn.BatchNorm1d(512),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(512,256),         # (N, 500) -> (N, 100)        \n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(256,128),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(128,10)\n",
        "        )\n",
        "        self.fc_model2 = nn.Sequential(\n",
        "              nn.Linear(1024,512),\n",
        "              nn.BatchNorm1d(512,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "              nn.Dropout(0.2),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(512,256),\n",
        "              nn.BatchNorm1d(256,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "              nn.Dropout(0.2),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(256,128),\n",
        "              nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "              nn.Dropout(0.2),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(128,10)\n",
        "# #             nn.Linear(1024,1024),          # (N, 1200) -> (N, 600)\n",
        "#             nn.BatchNorm1d(1024),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(1024,512),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(512,128),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(128,10)\n",
        "\n",
        "        )\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVipxV1rpfGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class vggNet(LinearNet):\n",
        "    def __init__(self): \n",
        "        super(vggNet, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = vgg(x)\n",
        "#         x1.requires_grad\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_model(x)\n",
        "        \n",
        "#         x1 = self.fc_model2(x1)\n",
        "#         x2 = vgg(x)\n",
        "#         x2 = x2.view(x2.size(0), -1)\n",
        "        x2= self.fc_model2(x)\n",
        "        x= self.fc_model1(x)\n",
        "        return torch.stack([x,x2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRFsmXKi155c",
        "colab_type": "code",
        "outputId": "6b2ff13d-09a3-417e-9d78-2b2e858a102d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "alexnet = models.alexnet(pretrained=True)\n",
        "print(alexnet)\n",
        "for param in alexnet.parameters():\n",
        "    param.requires_grad = False\n",
        "new_classifier = nn.Sequential(*list(alexnet.classifier.children())[:-7])\n",
        "alexnet.classifier = new_classifier\n",
        "print(alexnet)\n",
        "alexnet= alexnet.to(device)\n",
        "print(alexnet)    "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential()\n",
            ")\n",
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LzJU4QW2L4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearAlexNet(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(LinearAlexNet, self).__init__()\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(9216,4096),         # (N, 2704) -> (N, 1200)\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(4096,1024),         # (N, 2704) -> (N, 1200)\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.fc_model1 = nn.Sequential(\n",
        "            nn.Linear(1024,512),         # (N, 1000) -> (N, 500)\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512,10)         # (N, 500) -> (N, 100)        \n",
        "        )\n",
        "        self.fc_model2 = nn.Sequential(\n",
        "            nn.Linear(1024,10)          # (N, 1200) -> (N, 600)\n",
        "        )        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA9e5C8n1waq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class alexNet(LinearAlexNet):\n",
        "    def __init__(self): \n",
        "        super(alexNet, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = alexnet(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_model(x)\n",
        "        x2= self.fc_model1(x)\n",
        "        x = self.fc_model2(x)\n",
        "        return torch.stack([x,x2])     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPqAy0RTLbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModel(_imported, _loaded):\n",
        "  global net\n",
        "  if (not _imported):\n",
        "    kaggleImport()\n",
        "  if (not _loaded):\n",
        "    loadData()\n",
        "  transform, train_loader, validation_loader = prepareTrainingData()\n",
        "  \n",
        "  args= Params(\"LeakyReLU\", \"Adam\", \"VGG\", epochs)\n",
        "  net = LoadModel(args.model, args.activation_fn, args.optim)\n",
        "  PredictVowelsAndConsonants(args,net, train_loader, validation_loader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kNxJ4OCeVtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictOutput():\n",
        "  test_loader = prepareTestData(transform)\n",
        "  \n",
        "  print(test_loader, transform)\n",
        "  result_arr = predict(test_loader,net)\n",
        "  \n",
        "  print(result_arr[0])\n",
        "  submission = pd.DataFrame({'ImageId':result_arr[0], 'Class':[0]*result_arr[0].size})\n",
        "  submission = submission[['ImageId', 'Class']]\n",
        "  submission['Class'] = result_arr[1]\n",
        "  submission.to_csv(\"submission.csv\", index=False)\n",
        "  # submission.groupby('Class').size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSq7Wj_mvr2N",
        "colab_type": "text"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJahWCstvuf8",
        "colab_type": "code",
        "outputId": "7cbfdc73-cca8-41c6-ea00-098231c7f465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "trainModel(True, True)\n",
        "# print(_imported)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG LeakyReLU Adam\n",
            "CPU times: user 2 s, sys: 1e+03 ns, total: 3 s\n",
            "Wall time: 5.96 s\n",
            "Epoch: 0/10, Overall Train acc: 3.50,Overall Validation acc: 3.30\n",
            "Epoch: 1/10, Overall Train acc: 5.37,Overall Validation acc: 4.40\n",
            "Epoch: 2/10, Overall Train acc: 6.88,Overall Validation acc: 4.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5lVmUDN4zYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictOutput()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_8tfBiVzVBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}